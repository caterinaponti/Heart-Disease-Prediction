---
title: "Final Project"
author: "Caterina Ponti and Disha Khati"
date: "2024-11-06"
output: html_document
---

# Write Up:

**Question/Topic**

Can we build a predictive model to identify individuals at high risk of heart disease using patient data, and what are the most significant predictors of heart disease in this data set?

**Motivation**

Heart disease is a leading global health concern, responsible for significant mortality and morbidity. This data set offers an opportunity to address real-world problems by analyzing patient information to predict heart disease risk. By building an effective predictive model, we can enable early detection and prevention and personalized care.

**The Data set**

We analyzed the Data of Patients ( For Medical Field ) data set in Kaggle <https://www.kaggle.com/datasets/tarekmuhammed/patients-data-for-medical-field>.

**Challenges**

1.  The dataset exhibits a significant imbalance, with 224,429 observations for "No Heart Attack" compared to only 13,201 for "Had Heart Attack." This imbalance can hinder the performance of predictive models, as they may become biased toward the majority class, leading to poor sensitivity for detecting heart attack cases.

2.  When trying to predict whether a person had a heart attack, we found that the variable 'Had Angina' had a high predictive accuracy of 0.73. This suggests that the feature is potentially providing redundant, which could influence the model if not handled properly.

3.  The dataset has a large number of rows (237,630) and 35 variables (columns), leading to challenges with computational efficiency, overfitting, and the potential for irrelevant or noisy features to dilute the model's predictive power.

**How did you overcome the challenges?**

1.  To address the class imbalance, we employed downsampling techniques. This involves reducing the number of samples from the majority class ("No Heart Attack") to match the minority class ("Had Heart Attack"), thereby balancing the class distribution. This approach helps prevent the model from being biased toward the majority class and improves its ability to detect heart attack cases.

2.  To avoid overfitting and ensure the model is not overly influenced by 'Had Angina', we performed feature selection and examined the correlation between variables.

3.  To handle the large number of features and avoid overfitting, we applied techniques such as feature selection and sampling techniques to create a more manageable subset of the data for training.

**What packages were needed for your analysis?**

-   `readxl`: Used for reading and importing Excel files into R.
-   `ggplot2`:Essential for creating data visualizations such as histograms, scatter plots, and bar charts.
-   `pheatmap`: Utilized for creating heatmaps, especially useful for visualizing correlation matrices.
-   `dplyr`: For data manipulation (filtering and grouping).
-   `reshape2`: To reshape data.
-   `randomForest`: To implement Random Forest Regression for Variable Importance.
-   `varImp`: To assess variable importance.
-   `caret`: To build predictive models, data splitting, training and validation.
-   `cluster`: To perform hierarchical clustering.
-   `dendextend` - Used for visualizing dendograms created during hierarchical clustering.

**Discussion: What did you learn from this experience?**

From this experience, we learned the importance of performing Random Forest Regression for variable importance before fitting a Logistic Regression Model. The Random Forest method helped us identify which variables contributed most to predicting heart attacks, providing valuable insights. Additionally, hierarchical clustering allowed us to better understand the relationships between features and how they grouped in the data.

Regarding the dataset, we observed that fitting the model with all the variables gave us an accuracy of 0.79. However, the model's performance did not change significantly when we fitted it with only the most 5 important variables.

-   Accuracy with 'HadAngina': 0.73
-   Accuracy without 'HadAngina': 0.75
-   Accuracy of Top 5 variables without 'HadAngina'(AgeCategory, ChestScan, HadStroke, HadDiabetes, GeneralHealth): 0.72

This demonstrates that while 'HadAngina' is a strong predictor, other variables still contribute meaningfully to the model's overall predictive power.

Additionally, after performing hierchical clustering, we noticed that Cluster 4 had the highest proportion of heart attack cases and features in the cluster included: most individual had ChestScan, majority did not have a stroke but experienced difficulty in walking, and HadAngina and AgeCategory were key features.

**What more could you do with this project in the future?**

In the future, there are two directions to explore:

-   We could apply more advanced clustering techniques or adjust the number of clusters to see if any additional insights can be gained about the different patient groups.
-   Experimenting with deep learning techniques could improve prediction accuracy by using complex relationships between variables.

**Group Members' Contributions:**

Disha Khati: Exploratory Data Analysis (plots and data preparation), presentation slides, project's Write-Up.

Caterina Ponti: Model Building and Evaluation (Logistic Model Regression, Random Regression for Feature Importance and Hierarchical Clustering) and presentation slides. 


```{r}

library(readxl)
file.exists("/Users/caterinaponti/Desktop/BSDS100/Patients Data ( Used for Heart Disease Prediction ) 2.xlsx")
data <- read_excel("/Users/caterinaponti/Desktop/BSDS100/Patients Data ( Used for Heart Disease Prediction ) 2.xlsx")

head(data)
names(data)

```

```{r}
library(ggplot2)
library(dplyr)

patients_df <- data %>%
  mutate(HadDiabetes = ifelse(HadDiabetes == "Yes", 1, 0))

age_diabetes_risk <- aggregate(HadHeartAttack ~ AgeCategory + HadDiabetes, data = patients_df, FUN = mean)

# Plot the average risk by age category and diabetes status
ggplot(age_diabetes_risk, aes(x = factor(AgeCategory), y = HadHeartAttack, fill = factor(HadDiabetes))) +
  geom_bar(stat = "identity", position = "dodge") +  
  labs(
    title = "Average Risk of Heart Attack by Age Range and Diabetes Status",
    x = "Age Range",
    y = "Average Risk of Heart Attack",
    fill = "Had Diabetes"
  ) +
  scale_fill_manual(values = c("steelblue", "tomato")) +  
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
#Stacked Bar Plot for Health Conditions by Sex
ggplot(data, aes(x=factor(Sex), fill = factor(HadHeartAttack))) + geom_bar(position="stack") + labs(title= "Had Heart Attack by Sex", x = "Sex", fill= "Had Heart Attack")
```

```{r}
library(pheatmap) # For melting the correlation matrix
library(dplyr)
library(reshape2)

df_numeric <- data[sapply(data, is.numeric)]

corr <- cor(df_numeric)

corr_melted <- melt(corr)

# Plot the heatmap
pheatmap(corr, 
         color = colorRampPalette(c("blue", "white", "red"))(50), 
         display_numbers = TRUE,   
         fontsize_number = 10,    
         main = "Correlation Heatmap with Values")

```

```{r}
#who had a heart attack 
class_counts <- table(data$HadHeartAttack)
print(class_counts)
```

```{r}
class_0 <- data %>% filter(HadHeartAttack == 0)
class_1 <- data %>% filter(HadHeartAttack == 1)

```

```{r}
class_0_downsampled <- class_0[sample(nrow(class_0), size = nrow(class_1)), ]

balanced_data <- bind_rows(class_0_downsampled, class_1)

table(balanced_data$HadHeartAttack)
```

#Facet Grid for BMI by Smoking Status and Health Conditions

```{r}
ggplot(balanced_data, aes(x=BMI, fill=factor(SmokerStatus))) + geom_histogram(bins=20) + facet_wrap(~ factor(HadHeartAttack)) + labs(
  title="BMI Distribution by Smoking Status and Heart Attack", 
  x = "BMI", 
  fill = "Smoking Status"
)

```

```{r}
library(randomForest)
library(varImp)

balanced_data$HadHeartAttack <- as.factor(balanced_data$HadHeartAttack)
balanced_data <- subset(balanced_data, select=-c(PatientID, State))

rf_model <- randomForest(HadHeartAttack ~ ., data = balanced_data, importance = TRUE, mtry=6, ntree=100)

importance_values <- importance(rf_model)

sorted_by_accuracy <- importance_values[order(-importance_values[, "MeanDecreaseAccuracy"]), ]

varImpPlot(rf_model, n.var = 15, main="Variables Importance in Predicting Heart Attack")
```

```{r}

subset_data <- df_numeric[,c( 'HadHeartAttack', 'HadArthritis', 'DeafOrHardOfHearing', 'BMI','HadAngina', 'HadStroke', 'HadAsthma', 'HadCOPD','DifficultyConcentrating', 'DifficultyWalking', 'ChestScan', 'HighRiskLastYear', 'AlcoholDrinkers','CovidPos' )]

corr <- cor(subset_data)

corr_melted <- melt(corr)

# Plot the heatmap
pheatmap(corr, 
         color = colorRampPalette(c("blue", "white", "red"))(50), 
         display_numbers = TRUE,   
         fontsize_number = 10,    
         main = "Correlation Heatmap with Values")

```

```{r}
subset_data2 <- balanced_data[,c('DifficultyConcentrating', 'DifficultyWalking',
       'DifficultyDressingBathing', 'DifficultyErrands','ChestScan','HighRiskLastYear', 'CovidPos','AlcoholDrinkers', 'HIVTesting', 'FluVaxLast12', 'PneumoVaxEver')]

corr <- cor(subset_data2)

corr_melted <- melt(corr)

# Plot the heatmap
pheatmap(corr, 
         color = colorRampPalette(c("blue", "white", "red"))(50), 
         display_numbers = TRUE,   
         fontsize_number = 10,    
         main = "Correlation Heatmap with Values")

```

```{r}
#select categorical columns
categorical_cols <- names(balanced_data)[sapply(balanced_data, is.character)]

# Apply encoding
for (col in categorical_cols) {
  balanced_data[[col]] <- as.integer(factor(balanced_data[[col]]))
}


```

```{r}
#Define Target Variables and Features
X <- subset(balanced_data, select = -HadHeartAttack)
y <- balanced_data$HadHeartAttack

```

```{r}
library(caret)

set.seed(42)

train_indices <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[train_indices, ]
X_test <- X[-train_indices, ]
y_train <- y[train_indices]
y_test <- y[-train_indices]
```

**Fitting a Logistic Regression Model against all variables**

```{r}
suppressWarnings({model <- train(x = X_train, y = y_train, method = "glm", family = "binomial")})

predictions <- predict(model, newdata = X_test)

accuracy <- mean(predictions == y_test)
print(paste("Accuracy: ", accuracy))
#Accuracy:  0.789204545454545

conf_matrix <- confusionMatrix(predictions, y_test)
print(conf_matrix)

```

```{r}
#top 10 for variable importance
X_2 <- balanced_data[,c('HadAngina', 'AgeCategory', 'ChestScan', 'HadStroke', 'DifficultyWalking', 'Sex', 'HadDiabetes', 'WeightInKilograms', 'GeneralHealth', 'HeightInMeters')]

y <- balanced_data$HadHeartAttack

set.seed(43)

train_indices <- createDataPartition(y, p = 0.8, list = FALSE)
X_train2 <- X_2[train_indices, ]
X_test2 <- X_2[-train_indices, ]
y_train2 <- y[train_indices]
y_test2 <- y[-train_indices]

```

**Fitting a Logistic Regression Model with top 10 variables for importance**

```{r}
suppressWarnings({model2 <- train(x = X_train2, y = y_train2, method = "glm", family = "binomial")})

predictions2 <- predict(model2, newdata = X_test2)

accuracy2 <- mean(predictions2 == y_test2)
print(paste("Accuracy: ", accuracy2))
#Accuracy:  0.790530303030303

conf_matrix2 <- confusionMatrix(predictions2, y_test2)
print(conf_matrix2)

```

```{r}
#top 5 for variable importance
X_3 <- balanced_data[,c('HadAngina', 'AgeCategory', 'ChestScan', 'HadStroke', 'DifficultyWalking')]

y <- balanced_data$HadHeartAttack

set.seed(43)

train_indices <- createDataPartition(y, p = 0.8, list = FALSE)
X_train3 <- X_3[train_indices, ]
X_test3 <- X_3[-train_indices, ]
y_train3 <- y[train_indices]
y_test3 <- y[-train_indices]

```

**Fitting a Logistic Regression Model with top 5 variables for importance**

```{r}
suppressWarnings({model3 <- train(x = X_train3, y = y_train3, method = "glm", family = "binomial")})

predictions3 <- predict(model3, newdata = X_test3)

accuracy3 <- mean(predictions3 == y_test3)
print(paste("Accuracy: ", accuracy3))
#Accuracy:  0.78030303030303

conf_matrix3 <- confusionMatrix(predictions3, y_test3)
print(conf_matrix3)

```

**Fitted a Logistic Regression Model with only HadAngina to predict HadHeartAttack**

```{r}
#Only HadAngina
X_4 <- balanced_data[,c('HadAngina')]

y <- balanced_data$HadHeartAttack

set.seed(43)

train_indices <- createDataPartition(y, p = 0.8, list = FALSE)
X_train4 <- X_4[train_indices, ]
X_test4 <- X_4[-train_indices, ]
y_train4 <- y[train_indices]
y_test4 <- y[-train_indices]

suppressWarnings({model4 <- train(x = X_train4, y = y_train4, method = "glm", family = "binomial")})

predictions4 <- predict(model4, newdata = X_test4)

accuracy4 <- mean(predictions4 == y_test4)
print(paste("Accuracy: ", accuracy4))
#Accuracy:  0.734469696969697

conf_matrix4 <- confusionMatrix(predictions4, y_test4)
print(conf_matrix4)

```

**Fitted a Logistic Regression model with all the variables except HadAngina to predict HadHeartAttack**

```{r}
#Fit the model without 'HadAngina'
X_5 <- subset(balanced_data, select=-c(HadAngina, HadHeartAttack))

y <- balanced_data$HadHeartAttack

set.seed(43)

train_indices <- createDataPartition(y, p = 0.8, list = FALSE)
X_train5 <- X_5[train_indices, ]
X_test5 <- X_5[-train_indices, ]
y_train5 <- y[train_indices]
y_test5 <- y[-train_indices]

suppressWarnings({model5 <- train(x = X_train5, y = y_train5, method = "glm", family = "binomial")})

predictions5 <- predict(model5, newdata = X_test5)

accuracy5 <- mean(predictions5 == y_test5)
print(paste("Accuracy: ", accuracy5))
#Accuracy:  0.750568181818182

conf_matrix5 <- confusionMatrix(predictions5, y_test5)
print(conf_matrix4)


```

**Fitted a Logistic Regression model with top 5 variables except HadAngina to predict HadHeartAttack**

```{r}
X_6 <- balanced_data[,c('AgeCategory', 'ChestScan', 'HadStroke', 'HadDiabetes', 'GeneralHealth')]

y <- balanced_data$HadHeartAttack

set.seed(43)

train_indices <- createDataPartition(y, p = 0.8, list = FALSE)
X_train6 <- X_6[train_indices, ]
X_test6 <- X_6[-train_indices, ]
y_train6 <- y[train_indices]
y_test6 <- y[-train_indices]

suppressWarnings({model6 <- train(x = X_train6, y = y_train6, method = "glm", family = "binomial")})

predictions6 <- predict(model6, newdata = X_test6)

accuracy6 <- mean(predictions6 == y_test6)
print(paste("Accuracy: ", accuracy6))
#Accuracy:  0.723106060606061

conf_matrix6 <- confusionMatrix(predictions6, y_test6)
print(conf_matrix6)

```

```{r}
#splitting the data set in male and females
#Female = 1, Male = 2 
female = balanced_data[balanced_data$Sex == 1, ]
male = balanced_data[balanced_data$Sex == 2, ]
dim(female)
dim(male)
```

**Fitting Logistic Regression for Female Dataset**

```{r}

X_female <-  subset(female, select = -HadHeartAttack)

y_female <- female$HadHeartAttack

set.seed(43)

train_indices <- createDataPartition(y_female, p = 0.8, list = FALSE)
X_train <- X_female[train_indices, ]
X_test <- X_female[-train_indices, ]
y_train <- y_female[train_indices]
y_test <- y_female[-train_indices]

suppressWarnings({model_female <- train(x = X_train, y = y_train, method = "glm", family = "binomial")})

predictions_female <- predict(model_female, newdata = X_test)

accuracy_female <- mean(predictions_female == y_test)
print(paste("Accuracy: ", accuracy_female))
# Accuracy:  0.802013422818792   

conf_matrix_female <- confusionMatrix(predictions_female, y_test)
print(conf_matrix_female)

```

**Fitting Logistic Regression for Male Dataset**

```{r}

X_male <-  subset(male, select = -HadHeartAttack)

y_male <- male$HadHeartAttack

set.seed(43)

train_indices <- createDataPartition(y_male, p = 0.8, list = FALSE)
X_train <- X_male[train_indices, ]
X_test <- X_male[-train_indices, ]
y_train <- y_male[train_indices]
y_test <- y_male[-train_indices]

suppressWarnings({model_male <- train(x = X_train, y = y_train, method = "glm", family = "binomial")})

predictions_male <- predict(model_male, newdata = X_test)

accuracy_male <- mean(predictions_male == y_test)
print(paste("Accuracy: ", accuracy_male))
#Accuracy:  0.795854922279793

conf_matrix_male <- confusionMatrix(predictions_male, y_test)
print(conf_matrix_male)

```

**Running Random Forest Regression for Variable Importance for Female**

```{r}
rf_model_female <- randomForest(HadHeartAttack ~ ., data = female, importance = TRUE, mtry=6, ntree=100)

importance_values_female <- importance(rf_model_female)
importance_values_female
sorted_by_accuracy_female <- importance_values_female[order(-importance_values_female[, "MeanDecreaseAccuracy"]), ]

varImpPlot(rf_model_female, n.var = 10, main="Variable Importance for Female")



```

**Running Random Forest Regression for Variable Importance for Male**

```{r}
male$HadHeartAttack <-as.factor(male$HadHeartAttack)
rf_model_male <- randomForest(HadHeartAttack ~ ., data = male, importance = TRUE, mtry=6, ntree=100)

importance_values_male <- importance(rf_model_male)
sorted_by_accuracy_male <- importance_values_male[order(-importance_values_male[, 1]), ]


varImpPlot(rf_model_male, n.var=10, main="Variable Importance for Male") 
```

```{r}
#Bar Plot for male and female importance comparison 

top_male <- head(rownames(sorted_by_accuracy_male), 10)

top_female <- head(rownames(sorted_by_accuracy_female), 10)

male_importance <- sorted_by_accuracy_male[seq(1, 10), "MeanDecreaseAccuracy"]

female_importance <- sorted_by_accuracy_female[seq(1, 10), "MeanDecreaseAccuracy"]

importance_data <- data.frame(
  Feature = rep(c(top_male, top_female), each = 1),
  Importance = c(male_importance, female_importance),
  Gender = rep(c("Male", "Female"), each=10)
)

# Plot
ggplot(importance_data, aes(x = reorder(Feature, Importance), y = Importance, fill = Gender)) + geom_bar(stat = "identity", position="dodge")+
  labs(title="Top 10 Feature Importance Comaprison (Female vs Male)", x = "Feature", y = "Feature Decrease Accuracy") 


```

**Hierarchical Clustering**

```{r}

#Load required libraries for clustering
library(cluster)

```

```{r}

balanced_subset <- balanced_data[,c('HadAngina', 'AgeCategory', 'ChestScan', 'HadStroke', 'DifficultyWalking', 'HadHeartAttack')]

balanced_subset <- balanced_subset[ sample(1:nrow(balanced_subset), size = 75),]
df_feature <- balanced_subset[, sapply(balanced_subset, is.numeric)]
# Standardize the data (z-score normalization)
df_feature <- scale(df_feature)

#distance matrix
distance_matrix <- dist(df_feature, method = "euclidean")

# Perform hierarchical clustering 
set.seed(123) 
hc <- hclust(distance_matrix, method = "ward.D2")

# Cut tree into clusters
k <- 5
clusters <- cutree(hc, k = k)

balanced_subset$Cluster <- clusters

```

```{r}
library(dendextend)

dend <- as.dendrogram(hc)

has_heart_attack <- ifelse(balanced_subset$HadHeartAttack == 1, "red", "blue")

dend <- dend %>% set("branches_k_color", k = k) %>%
  set("labels_colors", has_heart_attack) %>%
  set("labels_cex", 0.7)

plot(dend, main= "Hierarchical Clustering with Heart Attack") 

legend("topright", legend=c("Heart Attack", "No Heart Attack"), fill = c("red", "blue"))


```

```{r}
#Analyze proportion of HadHeartAttack within each cluster to understand how well the clusters separate individuals with and without heart attacks. 
table(balanced_subset$Cluster, balanced_subset$HadHeartAttack)
```

```{r}
#Summary Statistic of Clusters 

cluster_summary <- balanced_subset %>% 
  mutate(HadHeartAttack = as.numeric(HadHeartAttack)) %>%
  group_by(Cluster) %>% 
  summarize(across(c(HadHeartAttack, HadAngina, ChestScan, HadStroke, DifficultyWalking, AgeCategory), mean))

print(cluster_summary)
```
Cluster 4 has the highest rate of "HadHeartAttack" = 1.933333.


